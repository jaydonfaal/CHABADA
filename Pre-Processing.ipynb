{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36025e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaydonfaal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaydonfaal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from deep_translator import GoogleTranslator\n",
    "import nltk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Emoji removal\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002500-\\U00002BEF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Language detection\n",
    "def is_language(text, lang_code):\n",
    "    try:\n",
    "        return detect(text) == lang_code\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Generic processing function\n",
    "def preprocess_description(raw_description, lang_code):\n",
    "    if not isinstance(raw_description, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Keep only paragraphs in the specified language\n",
    "    paragraphs = raw_description.split('\\n')\n",
    "    lang_paragraphs = [p for p in paragraphs if is_language(p, lang_code)]\n",
    "    text = ' '.join(lang_paragraphs)\n",
    "\n",
    "    # Remove emojis, links, emails, HTML, numbers\n",
    "    text = remove_emojis(text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Translate to English\n",
    "    try:\n",
    "        text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Tokenize and clean\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in english_stopwords]\n",
    "\n",
    "    # Stem in English\n",
    "    english_stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [english_stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6fd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Spanish file\n",
    "df_spanish = pd.read_excel('./App Description/spanish_desc.xlsx')\n",
    "df_spanish['processed_description'] = df_spanish['Description'].apply(lambda x: preprocess_description(x, 'es'))\n",
    "df_spanish.to_csv('./Processed Descriptions/spanish_processed_desc.csv', index=False)\n",
    "\n",
    "# Process French file\n",
    "df_french = pd.read_excel('./App Description/french_desc.xlsx')\n",
    "df_french['processed_description'] = df_french['Description'].apply(lambda x: preprocess_description(x, 'fr'))\n",
    "df_french.to_csv('./Processed Descriptions/french_processed_desc.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d81d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
